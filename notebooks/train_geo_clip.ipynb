{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "def json_to_dataframe(file_path):\n",
    "        \"\"\"\n",
    "        Read a JSON file and convert it to a Pandas DataFrame.\n",
    "\n",
    "        Parameters:\n",
    "        - file_path: str, path to the JSON file.\n",
    "        - csv_file_name: str, name of the output CSV file (default is 'output.csv').\n",
    "\n",
    "        Returns:\n",
    "        - DataFrame containing the JSON data.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        print(df.head())\n",
    "\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Example usage of json_to_dataframe()\n",
    "    #file_path = \"Outscraper-20241011183106xs96_fine_dining_restaurant.json\"\n",
    "    #file_path = \"\"\n",
    "    #  path up^^^^^^\n",
    "    #df = json_to_dataframe(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          query          name              google_id  \\\n",
      "0  Jax District  Jax District  24.7451938,46.5353316   \n",
      "1  Jax District  Jax District  24.7451938,46.5353316   \n",
      "2  Jax District  Jax District  24.7451938,46.5353316   \n",
      "3  Jax District  Jax District  24.7451938,46.5353316   \n",
      "4  Jax District  Jax District  24.7451938,46.5353316   \n",
      "\n",
      "                      place_id  \\\n",
      "0  ChIJ5dLZmB_hLj4R-XYc8tPWpJ8   \n",
      "1  ChIJ5dLZmB_hLj4R-XYc8tPWpJ8   \n",
      "2  ChIJ5dLZmB_hLj4R-XYc8tPWpJ8   \n",
      "3  ChIJ5dLZmB_hLj4R-XYc8tPWpJ8   \n",
      "4  ChIJ5dLZmB_hLj4R-XYc8tPWpJ8   \n",
      "\n",
      "                                       location_link  \\\n",
      "0  https://maps.google.com/?cid=11503555553999484665   \n",
      "1  https://maps.google.com/?cid=11503555553999484665   \n",
      "2  https://maps.google.com/?cid=11503555553999484665   \n",
      "3  https://maps.google.com/?cid=11503555553999484665   \n",
      "4  https://maps.google.com/?cid=11503555553999484665   \n",
      "\n",
      "                                            photo_id  \\\n",
      "0  AdCG2DM2RR9zbOaySKJHoCaCpmSOaOMN7wr-HXKioAOcSG...   \n",
      "1  AdCG2DPm_X-yqOpx_Z6VIlwqwBQcLaV2RgQ9TRncVy9E4C...   \n",
      "2  AdCG2DOmKI9jvDfg5GVV95fcm1jHI8AW123TS5W6DiceP5...   \n",
      "3  AdCG2DNRjlqgt66r9coLSiMYz_xzqRThtRSdESSEV2DD1i...   \n",
      "4  AdCG2DMGTFVJf0-xdaOraFkMniMpa7PtfAsEmssNBpF3n1...   \n",
      "\n",
      "                                           photo_url  \\\n",
      "0  https://maps.googleapis.com/maps/api/place/pho...   \n",
      "1  https://maps.googleapis.com/maps/api/place/pho...   \n",
      "2  https://maps.googleapis.com/maps/api/place/pho...   \n",
      "3  https://maps.googleapis.com/maps/api/place/pho...   \n",
      "4  https://maps.googleapis.com/maps/api/place/pho...   \n",
      "\n",
      "                                       photo_url_big   latitude  longitude  \\\n",
      "0  https://maps.googleapis.com/maps/api/place/pho...  24.745194  46.535332   \n",
      "1  https://maps.googleapis.com/maps/api/place/pho...  24.745194  46.535332   \n",
      "2  https://maps.googleapis.com/maps/api/place/pho...  24.745194  46.535332   \n",
      "3  https://maps.googleapis.com/maps/api/place/pho...  24.745194  46.535332   \n",
      "4  https://maps.googleapis.com/maps/api/place/pho...  24.745194  46.535332   \n",
      "\n",
      "  photo_date photo_upload_source photo_source_video photo_tags photo_tag_ids  \\\n",
      "0       None                None               None       None          None   \n",
      "1       None                None               None       None          None   \n",
      "2       None                None               None       None          None   \n",
      "3       None                None               None       None          None   \n",
      "4       None                None               None       None          None   \n",
      "\n",
      "                                  original_photo_url  \n",
      "0  https://maps.googleapis.com/maps/api/place/pho...  \n",
      "1  https://maps.googleapis.com/maps/api/place/pho...  \n",
      "2  https://maps.googleapis.com/maps/api/place/pho...  \n",
      "3  https://maps.googleapis.com/maps/api/place/pho...  \n",
      "4  https://maps.googleapis.com/maps/api/place/pho...  \n"
     ]
    }
   ],
   "source": [
    "df = json_to_dataframe(\"/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/data/sample.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['query', 'name', 'google_id', 'place_id', 'location_link', 'photo_id',\n",
       "       'photo_url', 'photo_url_big', 'latitude', 'longitude', 'photo_date',\n",
       "       'photo_upload_source', 'photo_source_video', 'photo_tags',\n",
       "       'photo_tag_ids', 'original_photo_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_images_from_df(df, url_column='photo_url', lat_column='latitude', lon_column='latitude', save_folder='Images', save_csv=False, csv_file_name='output.csv'):\n",
    "        \"\"\"\n",
    "        Download images from a DataFrame and save them locally in a specified folder.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame containing image URLs, latitudes, and longitudes.\n",
    "        - url_column: str, name of the column containing the image URLs (default is 'photo_url').\n",
    "        - lat_column: str, name of the column containing the latitudes (default is 'LAT').\n",
    "        - lon_column: str, name of the column containing the longitudes (default is 'LON').\n",
    "        - save_folder: str, folder where the images will be saved (default is 'Images').\n",
    "\n",
    "        Returns:\n",
    "        - df: The updated DataFrame with the 'IMG_FILE' column.\n",
    "        \"\"\"\n",
    "        # Create a folder to save images if it doesn't already exist\n",
    "        os.makedirs(save_folder, exist_ok=True)\n",
    "        print(f\"Images will be saved in: {save_folder}\")\n",
    "\n",
    "        # Dictionary to count occurrences of each latitude/longitude combination\n",
    "        counter = {}\n",
    "\n",
    "        # Iterate through the DataFrame row by row\n",
    "        for index, row in df.iterrows():\n",
    "            # Extract the image URL, latitude, and longitude from the current row\n",
    "            image_url = row[url_column]\n",
    "            latitude = row[lat_column]   # Updated reference\n",
    "            longitude = row[lon_column]   # Updated reference\n",
    "\n",
    "            # Create a unique key for the latitude and longitude\n",
    "            location_key = f\"{latitude}_{longitude}\"\n",
    "\n",
    "            # Increment the counter for each unique location\n",
    "            if location_key in counter:\n",
    "                counter[location_key] += 1\n",
    "            else:\n",
    "                counter[location_key] = 1\n",
    "\n",
    "            # Define the image file name using latitude, longitude, and the counter\n",
    "            image_name = os.path.join(save_folder, f\"{latitude}_{longitude}_{counter[location_key]}.jpg\")\n",
    "\n",
    "            try:\n",
    "                # Send a GET request to the image URL with a timeout of 10 seconds\n",
    "                response = requests.get(image_url, timeout=10)\n",
    "\n",
    "                # Check if the response indicates success (status code 200)\n",
    "                if response.status_code == 200:\n",
    "                    # Save the image content to the specified file\n",
    "                    with open(image_name, 'wb') as file:\n",
    "                        file.write(response.content)\n",
    "                    print(f\"Downloaded: {image_name}\")\n",
    "\n",
    "                    # Update the 'IMG_FILE' column in the DataFrame with the saved image path\n",
    "                    df.at[index, 'IMG_FILE'] = image_name\n",
    "                elif response.status_code == 400:\n",
    "                    # Handle bad request errors\n",
    "                    print(f\"Skipping image from {image_url} (status code: {response.status_code})\")\n",
    "                else:\n",
    "                    # Handle other unsuccessful status codes\n",
    "                    print(f\"Failed to download image from {image_url} (status code: {response.status_code})\")\n",
    "            except Exception as e:\n",
    "                # Handle any exceptions that occur during the request\n",
    "                print(f\"Error downloading image from {image_url}: {e}\")\n",
    "\n",
    "        # If saving the updated DataFrame as a CSV file is desired, do that\n",
    "        if save_csv:\n",
    "            df.to_csv(csv_file_name, index=False)\n",
    "            print(f\"DataFrame saved to {csv_file_name}\")\n",
    "\n",
    "        return df\n",
    "    # Example DataFrame setup (replace this with the actual DataFrame)\n",
    "    # Call the function to download images from the DataFrame\n",
    "    #df_p = download_images_from_df(df, save_csv=True, csv_file_name='geo_pic.csv')\n",
    "    # Display the updated DataFrame\n",
    "    #print(df_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images will be saved in: Images\n",
      "Downloaded: Images/24.7451938_24.7451938_1.jpg\n",
      "Downloaded: Images/24.7451938_24.7451938_2.jpg\n",
      "Downloaded: Images/24.7451938_24.7451938_3.jpg\n",
      "Downloaded: Images/24.7451938_24.7451938_4.jpg\n",
      "Downloaded: Images/24.7451938_24.7451938_5.jpg\n",
      "Downloaded: Images/24.7451938_24.7451938_6.jpg\n",
      "Downloaded: Images/24.7451938_24.7451938_7.jpg\n",
      "Downloaded: Images/24.7451938_24.7451938_8.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_1.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_2.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_3.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_4.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_5.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_6.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_7.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_8.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_9.jpg\n",
      "Downloaded: Images/24.7368086_24.7368086_10.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_1.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_2.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_3.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_4.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_5.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_6.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_7.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_8.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_9.jpg\n",
      "Downloaded: Images/25.3170213_25.3170213_10.jpg\n",
      "Downloaded: Images/24.734111_24.734111_1.jpg\n",
      "Downloaded: Images/24.734111_24.734111_2.jpg\n",
      "Downloaded: Images/24.734111_24.734111_3.jpg\n",
      "Downloaded: Images/24.734111_24.734111_4.jpg\n",
      "Downloaded: Images/24.734111_24.734111_5.jpg\n",
      "Downloaded: Images/24.734111_24.734111_6.jpg\n",
      "Downloaded: Images/24.734111_24.734111_7.jpg\n",
      "Downloaded: Images/24.734111_24.734111_8.jpg\n",
      "Downloaded: Images/24.734111_24.734111_9.jpg\n",
      "Downloaded: Images/24.734111_24.734111_10.jpg\n",
      "Downloaded: Images/26.8037529_26.8037529_1.jpg\n",
      "Downloaded: Images/26.8037529_26.8037529_2.jpg\n",
      "Downloaded: Images/26.8037529_26.8037529_3.jpg\n",
      "Downloaded: Images/26.8037529_26.8037529_4.jpg\n",
      "Downloaded: Images/26.8037529_26.8037529_5.jpg\n"
     ]
    }
   ],
   "source": [
    "df = download_images_from_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_semi_clean_data(df, lat_column='latitude', lon_column='longitude', IMG_FILE_column='IMG_FILE', output_file='semi_clean_data.csv'):\n",
    "        \"\"\"\n",
    "        Save a DataFrame with only latitude, longitude, and image path to a CSV file in the current directory.\n",
    "\n",
    "        Parameters:\n",
    "        - df: DataFrame containing the original data.\n",
    "        - lat_column: str, name of the column containing latitudes (default is 'latitude').\n",
    "        - lon_column: str, name of the column containing longitudes (default is 'longitude').\n",
    "        - IMG_FILE_column: str, name of the column containing image paths (default is 'IMG_FILE').\n",
    "        - output_file: str, name of the output CSV file (default is 'semi_clean_data.csv').\n",
    "        \"\"\"\n",
    "        # select only the columns interested in\n",
    "        semi_clean_df = df[[lat_column, lon_column, IMG_FILE_column]]\n",
    "\n",
    "        # Get the current working directory  to save the file\n",
    "        current_directory = os.getcwd()\n",
    "\n",
    "        # Create the full path for the output file with the specified name\n",
    "        output_path = os.path.join(current_directory, output_file)\n",
    "\n",
    "        # Save the new DataFrame to a CSV file at the specified location\n",
    "        semi_clean_df.to_csv(output_path, index=False)\n",
    "        print(f\"Semi-clean data saved to {output_path}\")\n",
    "\n",
    "    # Example usage of save_semi_clean_data():\n",
    "    # I have 'geo_pic.csv' as my input file, which will be loaded into a DataFrame\n",
    "    #file_path = \"geo_pic.csv\"\n",
    "    #file_path = \"\"\n",
    "    # Load the DataFrame from the CSV file\n",
    "    #df = pd.read_csv(file_path)\n",
    "    # Call the function to save the semi-clean data  just created\n",
    "    #save_semi_clean_data(df_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-clean data saved to /Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/geoclip/semi_clean_data.csv\n"
     ]
    }
   ],
   "source": [
    "save_semi_clean_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your dataset\n",
    "csv_file_path = '/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/data/one_file.csv'\n",
    "images_folder_path = '/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/geoclip/Images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading image paths and coordinates: 1it [00:00, 301.38it/s]\n"
     ]
    }
   ],
   "source": [
    "from train.dataloader import *\n",
    "\n",
    "train_transform = img_train_transform()\n",
    "\n",
    "train_loader = GeoDataLoader(csv_file_path, images_folder_path, transform=img_train_transform())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dataset_folder': '/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/geoclip/Images', 'transform': Compose(\n",
      "    RandomResizedCrop(size=(224, 224), scale=(0.08, 1.0), ratio=(0.75, 1.3333), interpolation=bilinear, antialias=True)\n",
      "    RandomHorizontalFlip(p=0.5)\n",
      "    RandomApply(\n",
      "    p=0.8\n",
      "    ColorJitter(brightness=(0.6, 1.4), contrast=(0.6, 1.4), saturation=(0.6, 1.4), hue=(-0.1, 0.1))\n",
      ")\n",
      "    RandomGrayscale(p=0.2)\n",
      "    PILToTensor()\n",
      "    ConvertImageDtype()\n",
      "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
      "), 'images': [], 'coordinates': []}\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/geoclip/model/location_encoder.py:57: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(f\"{file_dir}/weights/location_encoder_weights.pth\"))\n",
      "/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/geoclip/model/GeoCLIP.py:37: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.image_encoder.mlp.load_state_dict(torch.load(f\"{self.weights_folder}/image_encoder_mlp_weights.pth\"))\n",
      "/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/geoclip/model/GeoCLIP.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.location_encoder.load_state_dict(torch.load(f\"{self.weights_folder}/location_encoder_weights.pth\"))\n",
      "/Users/remaalnssiry/code/Remafsa/geo-pic/geo-pic/geoclip/model/GeoCLIP.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.logit_scale = nn.Parameter(torch.load(f\"{self.weights_folder}/logit_scale_weights.pth\"))\n"
     ]
    }
   ],
   "source": [
    "from model import GeoCLIP\n",
    "model = GeoCLIP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn  # Importing nn for loss functions and models\n",
    "import torch.optim as optim  # Importing optim for optimizers\n",
    "from tqdm import tqdm  # Importing tqdm for progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 20  # Example batch size\n",
    "\n",
    "# Define your optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Adjust learning rate as needed\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 10  # Total number of epochs to train\n",
    "epoch = 10\n",
    "criterion = nn.CrossEntropyLoss()  # Initialize the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "train(train_loader, model, optimizer, epoch, batch_size, device, scheduler=None, criterion=nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo-pic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
